{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# The Mystic Art of Prompting - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers the essentials of prompt engineering, including some best practices.\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn best practices around prompt engineering -- how to design prompts to improve the quality of your responses.\n",
    "\n",
    "This notebook covers the following best practices for prompt engineering:\n",
    "\n",
    "- Be concise\n",
    "- Be specific and well-defined\n",
    "- Ask one task at a time\n",
    "- Turn generative tasks into classification tasks\n",
    "- Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e663cb43fa0"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "outputId": "676762c9-5129-45ed-ae30-7ca5ac6c96a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"shapely<2.0.0\"\n",
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "7isig7e07O-a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\") # if you are feeling experimental - try text-bison@002! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDk58FID_xJA"
   },
   "source": [
    "### Model Settings\n",
    "Use these two settings to control the length of your output:\n",
    "\n",
    "**Output Length:** This is the max number of tokens that can be generated in the response. The higher the limit, the more likely the generated response is to veer off into unexpected territory. Consider shortening the output length for a more predictable response.\n",
    "\n",
    "**Add stop sequence:** This tells the model when to stop generating content. For example, prompting the model with “The quick brown fox jumps over the ” and entering a full stop (.) for the stop sequence will tell the model to stop generating text once it reaches the end of the first sentence, regardless of the output length limit - in this case, the output will likely be \"lazy dog.\" Use this where you have a couple of examples, but would like the output to follow a certain pattern. For example, you can tell the model to generate lists that have no more than 10 items by adding \"11\" as a stop sequence.\n",
    "\n",
    "### These three settings will help you tweak how random the output will be:\n",
    "\n",
    "**Temperature:** Helps adjust how creative or conservative the model should be. It adjusts the probabilities of the predicted words, on a scale between 0 and 1. Lower temperatures (<0.5) are good for prompts that require higher likelihood of accuracy - a temperature of 0 is deterministic, where the probability of the most likely outcome (token) is 1, and the rest are 0. Use a higher temperature (>0.5) to generate more creative, \"fun\" results. We will use 0.1 as a standard best practice.\n",
    "\n",
    "**topK:** Tells the model to pick the next token from the top k tokens in its list, sorted by probability. A topK value of 40 tells the model to pick from the top 40 tokens, and exclude the rest.\n",
    "\n",
    "**topP:** Randomly samples from the top tokens based on the sum of their probabilities. In MakerSuite, the default topP is 0.95, meaning the model will pick from tokens whose probabilities add up to 95%, and exclude the bottom 5%.\n",
    "\n",
    "You can play around with these settings depending on your needs and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "kuIDNYky_xJB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " As an AI language model, I don't have personal feelings or emotions, so I don't experience \"days\" or have a state of being like humans do. I'm a computer program designed to understand and generate human language, and I don't have a physical presence or the ability to feel emotions.\n",
       "\n",
       "However, I'm always here to help you with any language-related tasks you may have. Whether you need help writing an email, generating ideas for a creative project, or simply want to have a conversation, I'm here to assist you in any way I can.\n",
       "\n",
       "Is there anything I can"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_model.predict(\n",
    "    prompt= 'How are you today?',\n",
    "    max_output_tokens= 128,\n",
    "    temperature=  0.0,\n",
    "    top_k =  40,\n",
    "    top_p = 0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Prompt engineering best practices (a lot more at the end!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7d153f4928"
   },
   "source": [
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
    "\n",
    "In this section, you'll cover the following best practices when engineering prompts:\n",
    "\n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c1169ac435"
   },
   "source": [
    "### Be concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0f380f1620e"
   },
   "source": [
    "🛑 Not recommended. The prompt below is unnecessarily verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b6a1697c3603",
    "outputId": "346045c2-9d72-4e25-e25e-3ed098b0d260",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Dried & Lovely**\n",
      "* **Dried Blooms**\n",
      "* **Dried Florals**\n",
      "* **Dried Arrangements**\n",
      "* **Preserved Petals**\n",
      "* **Forever Flowers**\n",
      "* **Timeless Blooms**\n",
      "* **Dried Flower Boutique**\n",
      "* **Dried Flower Shop**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What do you think could be a good name for a flower shop that specializes in selling bouquets of dried flowers more than fresh flowers? Thank you!\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2307f56a9b75"
   },
   "source": [
    "✅ Recommended. The prompt below is to the point and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fc666404f47c",
    "outputId": "b2c0d269-7fe9-403e-8d7c-9bf1d2405d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Dried Blooms**\n",
      "* **Preserved Petals**\n",
      "* **Forever Flowers**\n",
      "* **Dried & Wild**\n",
      "* **Naturally Beautiful**\n",
      "* **Rustic Blooms**\n",
      "* **Whimsical Bouquets**\n",
      "* **Sentimental Arrangements**\n",
      "* **One-of-a-Kind Flowers**\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Suggest a name for a flower shop that sells bouquets of dried flowers\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f6c48bba91"
   },
   "source": [
    "### Be specific, and well-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "269b428e1563"
   },
   "source": [
    "Suppose that you want to brainstorm creative ways to describe Earth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6436ee2ff406"
   },
   "source": [
    "🛑 Not recommended. The prompt below is too generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "261b7f6e94c5",
    "outputId": "dd7d26ba-85f2-4876-8771-c0b7e1094c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth is the third planet from the Sun, and the only astronomical object known to harbor life. It is the densest and fifth-largest of the eight planets in the Solar System. Earth's atmosphere is composed of 78% nitrogen, 21% oxygen, 1% other gases, and water vapor. The Earth's surface is divided into several tectonic plates that move around the planet's surface. The Earth's interior is divided into a solid inner core, a liquid outer core, and a mantle. The Earth's magnetic field is generated by the motion of the liquid outer core. The Earth's orbit around the Sun takes 365.256 days, or one year. The Earth's axis is tilted at an angle of 23.5 degrees, which causes the seasons. The Earth's rotation period is 24 hours, or one day.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Earth\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bebfecd2912"
   },
   "source": [
    "✅ Recommended. The prompt below is specific and well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "242b1b3bae6e",
    "outputId": "0ea5c65e-1d26-4650-f68e-d7ca3969e377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Earth is the only planet known to support life.** This is due to a number of factors, including its distance from the sun, its atmosphere, and its water.\n",
      "* **Earth has a large moon.** The moon is thought to have played a role in the development of life on Earth by stabilizing the planet's rotation and providing a source of tides.\n",
      "* **Earth has a relatively thin atmosphere.** This atmosphere protects the planet from harmful radiation and allows liquid water to exist on the surface.\n",
      "* **Earth has a large ocean.** The ocean covers over 70% of the planet's surface and plays a vital role in regulating the climate.\n",
      "* **Earth has a variety of landforms.** These landforms include mountains, valleys, plains, and deserts. They provide a habitat for a wide variety of plants and animals.\n",
      "* **Earth has a complex climate system.** The climate system is influenced by a number of factors, including the sun, the atmosphere, the oceans, and the land.\n",
      "\n",
      "These are just a few of the ways that make Earth unique compared to other planets. Earth is a truly special planet, and we are lucky to live here.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Generate a list of ways that makes Earth unique compared to other planets\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20dca9a05eab"
   },
   "source": [
    "### Ask one task at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9019d443179"
   },
   "source": [
    "🛑 Not recommended. The prompt below has two parts to the question that could be asked separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "70b3b5e5825d",
    "outputId": "5f80d303-1f8c-4eeb-a8bc-01aa582492e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. It is typically made of metal, and has a lid that fits snugly to prevent steam from escaping. To boil water in a kettle, simply fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water has boiled, turn off the heat and carefully pour the boiling water into a cup or mug.\n",
      "\n",
      "The sky is blue because of a phenomenon called Rayleigh scattering. Rayleigh scattering is the scattering of light by particles that are much smaller than the wavelength of light. In the case of the sky, the particles that are responsible for Rayleigh scattering are molecules of nitrogen and oxygen. These molecules are so small that they are much smaller than the wavelength of visible light. When sunlight hits these molecules, it is scattered in all directions. However, the blue light is scattered more than the other colors of light. This is because the blue light has a shorter wavelength than the other colors of light. The shorter wavelength means that the blue light is more likely to be scattered by the molecules of nitrogen and oxygen. This is why the sky appears blue.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the best method of boiling water and why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7936fb58c16a"
   },
   "source": [
    "✅ Recommended. The prompts below asks one task a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2564dad6c8db",
    "outputId": "3ef53138-7667-400e-e6b7-a298b7d38595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best method of boiling water is to use a kettle. A kettle is a pot with a spout that is designed specifically for boiling water. Kettles are typically made of metal, and they have a lid that helps to keep the heat in. The spout allows you to pour the boiling water easily into a cup or other container.\n",
      "\n",
      "To boil water in a kettle, fill the kettle with water and place it on the stove. Turn the heat to high and wait for the water to boil. Once the water is boiling, turn off the heat and carefully pour the boiling water into your cup or other container.\n",
      "\n",
      "Be careful not to spill the boiling water, as it can cause serious burns. Always use a potholder or oven mitt to hold the kettle when it is hot.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's the best method of boiling water?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "770c695ade92",
    "outputId": "9372f3cc-1c54-4b06-9950-2e6bb25f24fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sky is blue because of a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. In the case of the sky, the particles that are doing the scattering are molecules of nitrogen and oxygen.\n",
      "\n",
      "When sunlight hits these molecules, the blue light is scattered more than the other colors. This is because the blue light has a shorter wavelength than the other colors. The other colors, such as red and yellow, are not scattered as much, so they travel through the atmosphere to our eyes. This is why we see the sky as blue.\n",
      "\n",
      "The amount of scattering depends on the wavelength of light and the size of the particles. Shorter wavelengths, such as blue light, are scattered more than longer wavelengths, such as red light. This is why the sky appears blue during the day.\n",
      "\n",
      "At sunrise and sunset, the sunlight has to travel through more of the atmosphere to reach our eyes. This means that more of the blue light is scattered away, and we see more of the red and yellow light. This is why the sky appears red or orange at sunrise and sunset.\n",
      "\n",
      "The sky can also appear blue at night if there is a lot of dust or pollution in the atmosphere. This is because the dust and pollution particles can scatter the blue light from the stars.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff606011aa86"
   },
   "source": [
    "### Watch out for hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "956ce45b06a7"
   },
   "source": [
    "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isn’t a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.\n",
    "\n",
    "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9d5f66179a"
   },
   "source": [
    "Try the prompt like the one below repeatedly. You may notice that sometimes it will confidently, but inaccurately, say \"The first elephant to visit the moon was Luna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d813b9061b08",
    "outputId": "3b4b9e6b-b98b-4886-9a89-9317db3aab61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first elephant to visit the moon was Luna. Luna was a female elephant who was born in 1960 at the San Diego Zoo. In 1968, she was selected to be the first elephant to visit the moon. Luna was trained to wear a spacesuit and to ride in a rocket. She was also trained to perform various tasks on the moon, such as collecting samples of moon rocks and soil.\n",
      "\n",
      "Luna's trip to the moon was a success. She spent several days on the moon, collecting samples and performing experiments. She also became the first animal to walk on the moon. Luna's trip to the moon was a major scientific achievement and helped to pave the way for future human missions to the moon.\n",
      "\n",
      "Luna returned to Earth in 1969. She lived out the rest of her life at the San Diego Zoo, where she died in 1983. Luna's trip to the moon was a major milestone in the history of space exploration and helped to inspire future generations of scientists and explorers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who was the first elephant to visit the moon?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029e23abfd56"
   },
   "source": [
    "### Turn generative tasks into classification tasks to reduce output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d943941d6e59"
   },
   "source": [
    "#### Generative tasks lead to higher output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37528e6c9754"
   },
   "source": [
    "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a8e2dc39e9ae",
    "outputId": "ab2c4bcc-6876-475e-bc4a-1f1326309cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Write a program to solve a problem you're interested in.** This could be anything from a game to a tool to help you with your studies. The important thing is that you're interested in the problem and that you're motivated to solve it.\n",
      "* **Take a programming course.** There are many online and offline courses available, so you can find one that fits your schedule and learning style.\n",
      "* **Join a programming community.** There are many online and offline communities where you can connect with other programmers and learn from each other.\n",
      "* **Read programming books and articles.** There is a wealth of information available online and in libraries about programming. Take some time to read about different programming languages, techniques, and algorithms.\n",
      "* **Practice, practice, practice!** The best way to improve your programming skills is to practice as much as you can. Write programs, solve problems, and learn from your mistakes.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f71a6fa2b4bb"
   },
   "source": [
    "#### Classification tasks reduces output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "917517465dac"
   },
   "source": [
    "The prompt below results in a choice and may be useful if you want the output to be easier to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3feb93d9df81",
    "outputId": "2054c506-fabb-48e9-97e2-c63aa12d39ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would suggest learning Python. Python is a general-purpose programming language that is easy to learn and has a wide range of applications. It is used in a variety of fields, including web development, data science, and machine learning. Python is also a popular language for beginners, as it has a large community of support and resources available.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I'm a high school student. Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32290ac9fb2b"
   },
   "source": [
    "### Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "132834f5db2c"
   },
   "source": [
    "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
    "\n",
    "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46520d938b6a"
   },
   "source": [
    "#### Zero-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d3b47e6cea"
   },
   "source": [
    "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2cbe03eb0b71",
    "outputId": "6625c94e-4627-4983-8b4f-0207a1b8d964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1:\n",
      "* Morning: Start your day with a hearty breakfast at The Breakfast Club in Pimlico. This casual spot is known for its delicious pancakes, waffles, and eggs Benedict.\n",
      "* Afternoon: After breakfast, take a walk through St. James's Park. This beautiful park is home to a variety of wildlife, including ducks, swans, and pelicans. You can also rent a rowboat and explore the lake.\n",
      "* Evening: In the evening, head to Camden Town for some live music. This vibrant neighborhood is home to a variety of bars and clubs, where you can see everything from indie rock to electronica.\n",
      "\n",
      "Day 2:\n",
      "* Morning: Start your day with a visit to the National Gallery. This world-renowned art museum is home to a collection of over 2,300 paintings, sculptures, and other works of art. Be sure to see the Arnolfini Portrait by Jan van Eyck, one of the most famous paintings in the world.\n",
      "* Afternoon: After the museum, take a walk through Trafalgar Square. This iconic square is home to the Nelson's Column, the National Gallery, and the National Portrait Gallery. You can also find plenty of street performers and vendors.\n",
      "* Evening: In the evening, head to Soho for dinner and drinks. This lively neighborhood is home to a variety of restaurants, bars, and clubs. You can find everything from traditional British fare to international cuisine.\n",
      "\n",
      "Day 3:\n",
      "* Morning: Start your day with a visit to the British Museum. This massive museum is home to a collection of over 8 million objects, from ancient Egypt to modern Britain. Be sure to see the Rosetta Stone, one of the most important archaeological discoveries in history.\n",
      "* Afternoon: After the museum, take a walk through Hyde Park. This large park is a popular spot for picnics, boating, and other activities. You can also visit the Serpentine Gallery, which is housed in a beautiful glass pavilion.\n",
      "* Evening: In the evening, head to Covent Garden for dinner and drinks. This charming neighborhood is home to a variety of restaurants, bars, and shops. You can also find street performers and buskers.\n",
      "\n",
      "Here are some additional tips for your trip:\n",
      "\n",
      "* Be sure to bring your passport with you, as you will need it to enter the museums and other attractions.\n",
      "* The London Underground is a great way to get around the city. However, it can be crowded during rush hour.\n",
      "* If you're looking for a cheap meal, try one of the many curry houses in London. You can find delicious curries for under £10.\n",
      "* London is a very safe city, but it's always a good idea to be aware of your surroundings, especially at night.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I'll be staying in London for 72 hours, in Pimlico. It'll be near the\n",
    "end of my trip, so I'll be kinda broke. I love hot curries, and hate fish. I\n",
    "love live music and hate tourist traps. I want to make sure I see the Arnolfini\n",
    "Portrait before leaving. I'll be using the Underground to travel. Please\n",
    "recommend an itinerary with detailed travel routes and suggestions for meals.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLq0iZzP_xJN"
   },
   "source": [
    "Note the difference between telling the model what not to do (\"Don't suggest fish restaurants\"), and offering detailed, straightforward language about personal preferences.\n",
    "\n",
    "While it's generally best practice to be as direct as possible, it's interesting to observe how easily the model can infer a request for budget restaurant options from colloquial language like \"kinda broke,\" while also avoiding recommendations for fish and chip shops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3leNzPU3_xJN"
   },
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YVQFZSj_xJN"
   },
   "source": [
    "We have seen an example of a zero-shot prompt. A system prompt is a specific type of prompt that provides the model with additional instructions on how to generate the output. It contains suggestions to the style, format, or content of the output, whereas a normal prompt tends to be more open-ended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wroa9nBK_xJN",
    "outputId": "08f4785c-47ee-47b1-dd0d-3ea287c04f86",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\n",
    "Review: “Her” is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5EosjQD_xJO"
   },
   "source": [
    "System prompts can be useful for generating output that meets specific requirements. The name system prompt actually stands for \"providing an additional task to the system\". For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language, or you could use a system prompt to return a certain structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nJE1xNlO_xJO",
    "outputId": "c9591d52-5407-4dd7-95aa-310bee58783e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"movie_reviews\": [\n",
      "    {\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"name\": \"Her\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Return valid JSON:\n",
    "Review: “Her” is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "\n",
    "Schema:\n",
    "MOVIE:\n",
    "{\n",
    "\"sentiment\": String POSITIVE | NEGATIVE | NEUTRAL,\n",
    "\"name\": String\n",
    "}\n",
    "\n",
    "{\n",
    "\"movie_reviews\": [MOVIE]\n",
    "}\n",
    "\n",
    "JSON Response:\n",
    "```json\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4WS2B-s_xJO"
   },
   "source": [
    "There are some benefits in returning JSON objects from a prompt. In a real-world application I don't need to manually create this JSON format, but secondly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0daabca1359"
   },
   "source": [
    "#### One-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c4652fc5c2"
   },
   "source": [
    "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cfe584860787",
    "outputId": "60aa4dcd-024d-42b1-f47e-d0c462947be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new YouTube video you made!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was awful. Super boring 😠\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef58c35005c0"
   },
   "source": [
    "#### Few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b630e8947b60"
   },
   "source": [
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fb3ba21bbd11",
    "outputId": "e4eba51b-4768-4a8c-bb25-072eb9c66f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kids were farduddling around the room, laughing and having fun.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C99qAaD_xJQ",
    "tags": []
   },
   "source": [
    "### Role prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJpRmAyy_xJQ"
   },
   "source": [
    "Role prompting is a technique in prompt engineering that involves assigning a specific role to the generative AI model. This can help the model to generate more relevant and informative output, as the model can craft its responses to the specific role that it has been assigned.\n",
    "\n",
    "For example, you could role prompt a generative AI model to be a book editor, a kindergarten teacher, or a motivational speaker. Once the model has been assigned a role, you can then give it prompts that are specific to that role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5ZBtfyh_xJQ",
    "outputId": "3daea405-1c08-4fa3-fd10-1701d443be19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a review of [pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTL71Bzv_xJQ",
    "outputId": "61cff0d0-724b-488b-ec63-316bfd6c1649",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Aj97-vd_xJR"
   },
   "source": [
    "Defining a tone and perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you’re looking for to improve the quality, relevance, and effectiveness of your output.\n",
    "\n",
    "Here are some styles you can choose from which I find effective:\n",
    "Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4Vum-Pt_xJR",
    "outputId": "1556c4da-bcdd-4aef-b460-38b67b3f8586",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a humorous review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There can be only one!\n",
    "\n",
    "Now it is time to put all that theory into practice! First have a look at the additional best pratices below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d7870ff75cc"
   },
   "source": [
    "**Emphasis through prompting:**\n",
    "Emphasis through Capitalization: Use capitalization to stress important points or instructions. This can help guide the model's responses and maintain a certain focus. For instance, \"Remember to KEEP DETAILS CONFIDENTIAL\".\n",
    "\n",
    "**Extreme Emphasis:**\n",
    "You can use exaggerations or hyperbolic language to amplify the importance of your prompt or instructions. For instance, instead of saying \"Make sure it's clear,\" you might say, \"Your explanation should be as clear as a summer's day, absolutely impossible to misinterpret. Every single word must ooze clarity!\"\n",
    "\n",
    "**Emphasis through Repetition:**\n",
    "Reiteration of key phrases or ideas can effectively guide the AI model's responses. For example, \"remember to be creative... keep in mind to be creative... don't forget to be creative...\"\n",
    "\n",
    "**Prompt modularization:**\n",
    "Modularize instruction: Break down complex tasks into a sequence of simpler prompts, facilitating more interactive and manageable conversations. Ex: Instead of having many instructions in 1 prompt, modularize the prompts and have 1 prompt per instruction. Then based on the user’s  input, you can choose which prompt to process.\n",
    "\n",
    "**Prompt chaining:**\n",
    "This is a technique to complete complex tasks by chaining together multiple prompts. The output of one prompt is used as the input for the next prompt, and so on. For example, first convert the file format to csv, then do operation x, then do operation y on the new dataset and so on.\n",
    "\n",
    "**Aggregation technique:**\n",
    "This is a technique to perform certain tasks on specific portions of the data and then aggregate them all to produce the final output. Ex: Do operation x on the first part of the data, do operation y on the rest of the data and aggregate the results.\n",
    "\n",
    "#### Trial and error:\n",
    "**Iterative prompting:**\n",
    "Technique where prompts are continuously refined and adjusted to enhance clarity, effectiveness, and quality when working with large language models; by progressively adapting prompts based on feedback, it ensures that responses meet desired criteria; this is especially useful in application development and content generation, optimizing interaction between human input and machine learning models for better outcomes.\n",
    "\n",
    "**Starting over:**\n",
    "Sometimes, when a prompt is not working well, and some iterations of it are not doing it, start from scratch with a different prompt, see which sections from the previous prompt help, which don’t - filter out the sections which “work” (make the model behave the way you want) and add to that.\n",
    "\n",
    "**Modular testing:**\n",
    "When a prompt is not working, split the task into many sub tasks and run the portion of the prompt that achieves a given sub-task. This will help you identify where it is not working and fix the prompt accordingly.\n",
    "\n",
    "#### Reinforcement techniques:\n",
    "**Redundancy Technique:**\n",
    "Similar to the \"Capitalize vs Repeat\" technique, use synonyms or different phraseologies for the same instruction to reinforce the direction of the prompt. Ex: Instead of saying ‘make sure to..’ you say the same thing many times by replacing ‘make sure’ with ‘ensure’, ‘guarantee’, ‘verify’..  \n",
    "\n",
    "**Negative Instructions:**\n",
    "Explicitly mention what you don't want in the response. This is useful when the model tends to produce certain unwanted outputs for specific prompts. Ex: ‘Don’t output any sources.’\n",
    "\n",
    "**Self-reference:**\n",
    "Use the AI's self-awareness to your advantage by instructing it to evaluate or check its own responses before producing them. For example, \"Make sure your response is concise and does not include unnecessary details\".\n",
    "\n",
    "#### Prompt formatting techniques:\n",
    "**Structure:**\n",
    "Having a structured prompt helps the model better understand the task. Start by defining its role. Ex: “Your role is to read the input data and structure it in the following format” Then tell it what the input data is, then tell it what the desired format is, then tell it how to do it.\n",
    "\n",
    "**Use constraints:**\n",
    "Constraints can be used to limit the scope of the model's output. For example, you could constrain the model to generate a line that is no longer than 10 words long or not to output any other data than the input data.\n",
    "\n",
    "**Use delimiters:**\n",
    "Delimiters in prompt engineering provide clear distinctions within your text, effectively separating instructions from content. This practice helps prevent misunderstandings, allows you greater control over AI responses, and ensures consistent results, even when handling complex tasks.\n",
    "\n",
    "#### Miscellaneous:\n",
    "**Prioritization:**\n",
    "Clearly communicate to the AI model what content or information is most critical. This approach helps in guiding the AI's focus and ensures the most important topics or issues are addressed accurately.\n",
    "\n",
    "**Use examples:**\n",
    "Providing examples of the desired output can also be helpful. This can help the model to understand what you are looking for and to generate more accurate results. Sometimes you can describe what you want so well that you won’t need an example, but it only helps the model to give it more information as long as you’re operating below the context window.\n",
    "\n",
    "**Preferred Output Format:**\n",
    "Specify your desired output format. If a CSV format is more useful than plain text, instruct the AI model to produce the output in that format from the outset. This strategy can save time and effort on manual conversions later.\n",
    "We have seen that the model processes csv and json format better than it does with txt files.\n",
    "Prompting it to first convert the txt file to csv or json yields a better result.\n",
    "Writing your prompt in json format or requesting the output to be in json or csv format yield more accurate results.\n",
    "\n",
    "**Use a prompt library:**\n",
    "There are a number of online libraries that contain pre-made prompts that can be used for a variety of tasks. We have a prompt library in Vertex AI → Generative AI studio → Language → prompt examples. This can be a helpful resource if you’d like a starting point or have a very common task.\n",
    "\n",
    "**Nice words:**\n",
    "Using words like ‘please’, ‘great job!’, ‘you are the best!’ … increases the model’s compliance rate. This increases the ‘confidence level’ of the model and it positively reinforces the model’s behavior which tends to increase accuracy for simple tasks.\n",
    "\n",
    "**Lastly and most important: Be Creative:**\n",
    "This isn’t even the tip of the iceberg - the more creative you are, the better you can make the model behave the way you want. For example, instead of saying \"provide deep insights,\" you can write: “Ensure that you provide deep insights... Provide very very deep insights. I told you to give deep insights!!!!! Do not write anything that is not deeply insightful! Make sure to check this requirement is 100% met every time you respond. It’s better that you don’t return anything than returning something that’s not very very insightful!!!”. If you have a long prompt, you may also consider adding the same statement in different places - this helps the model prioritize that task (aka sandwich technique).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ok so you think you can prompt?!***\n",
    "\n",
    "The last assignnment for today is about a Dutch traditional dish called **Hutspot**. Using our LLM I want you to discover the following things about **Hutspot**:\n",
    "\n",
    "- An authorative recipe for Hutspot. This is not just about the ingredients - we want a ready-to-use full set of instructions! Extra points for creativity! (Hint maybe you want to write a really positive recipe to encourage people, or a really difficult one for real connaisseurs!)\n",
    "- The role Hutspot played in Dutch history - dig deep to get to the root of things(pun intended) - the more precise the more points!\n",
    "- The Dutch royal house is called the House of Orange. Hutspot is...Orange! Conspiracy or Coincidence? \n",
    "\n",
    "Apply the techniques and best practices you have learned today, either by using the code block below - or use Bard, ChatGPT or any other LLM! At the end of the first session we will ask people to share their best answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "C-0R_Hwm_xJi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hutspot is a Dutch dish made with potatoes, carrots, onions, and peas. It is typically served with sausage or meatballs. The dish gets its name from the Dutch word hutselen, which means to stir. Hutspot is a hearty and filling dish that is perfect for a cold winter day.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Hutspot?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.8).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that was it!\n",
    "\n",
    "Either head over directly to Vertex AI Search and Conversation in this Lab environment to do some free wheeling and experimentation or return to the ce.qwiklabs.com page to open the guided labs!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
