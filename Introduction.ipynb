{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "# The Mystic Art of Prompting - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers the essentials of prompt engineering, including some best practices.\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn best practices around prompt engineering -- how to design prompts to improve the quality of your responses.\n",
    "\n",
    "This notebook covers the following best practices for prompt engineering:\n",
    "\n",
    "- Be concise\n",
    "- Be specific and well-defined\n",
    "- Ask one task at a time\n",
    "- Turn generative tasks into classification tasks\n",
    "- Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e663cb43fa0"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "outputId": "676762c9-5129-45ed-ae30-7ca5ac6c96a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"shapely<2.0.0\"\n",
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "7isig7e07O-a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\") # if you are feeling experimental - try text-bison@002! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDk58FID_xJA"
   },
   "source": [
    "### Model Settings\n",
    "Use these two settings to control the length of your output:\n",
    "\n",
    "**Output Length:** This is the max number of tokens that can be generated in the response. The higher the limit, the more likely the generated response is to veer off into unexpected territory. Consider shortening the output length for a more predictable response.\n",
    "\n",
    "**Add stop sequence:** This tells the model when to stop generating content. For example, prompting the model with ‚ÄúThe quick brown fox jumps over the ‚Äù and entering a full stop (.) for the stop sequence will tell the model to stop generating text once it reaches the end of the first sentence, regardless of the output length limit - in this case, the output will likely be \"lazy dog.\" Use this where you have a couple of examples, but would like the output to follow a certain pattern. For example, you can tell the model to generate lists that have no more than 10 items by adding \"11\" as a stop sequence.\n",
    "\n",
    "### These three settings will help you tweak how random the output will be:\n",
    "\n",
    "**Temperature:** Helps adjust how creative or conservative the model should be. It adjusts the probabilities of the predicted words, on a scale between 0 and 1. Lower temperatures (<0.5) are good for prompts that require higher likelihood of accuracy - a temperature of 0 is deterministic, where the probability of the most likely outcome (token) is 1, and the rest are 0. Use a higher temperature (>0.5) to generate more creative, \"fun\" results. We will use 0.1 as a standard best practice.\n",
    "\n",
    "**topK:** Tells the model to pick the next token from the top k tokens in its list, sorted by probability. A topK value of 40 tells the model to pick from the top 40 tokens, and exclude the rest.\n",
    "\n",
    "**topP:** Randomly samples from the top tokens based on the sum of their probabilities. In MakerSuite, the default topP is 0.95, meaning the model will pick from tokens whose probabilities add up to 95%, and exclude the bottom 5%.\n",
    "\n",
    "You can play around with these settings depending on your needs and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "kuIDNYky_xJB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " As an AI language model, I don't have personal feelings or emotions, so I don't experience \"days\" or have a state of being like humans do. I'm a computer program designed to understand and generate human language, and I don't have a physical presence or the ability to feel emotions.\n",
       "\n",
       "However, I'm always here to help you with any language-related tasks you may have. Whether you need help writing an email, generating ideas for a creative project, or simply want to have a conversation, I'm here to assist you in any way I can.\n",
       "\n",
       "Is there anything I can"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_model.predict(\n",
    "    prompt= 'How are you today?',\n",
    "    max_output_tokens= 128,\n",
    "    temperature=  0.0,\n",
    "    top_k =  40,\n",
    "    top_p = 0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Prompt engineering best practices (a lot more at the end!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df7d153f4928"
   },
   "source": [
    "Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.\n",
    "\n",
    "The idea of using \"unfancy\" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer \"unfancy\" prompts.\n",
    "\n",
    "In this section, you'll cover the following best practices when engineering prompts:\n",
    "\n",
    "* Be concise\n",
    "* Be specific, and well-defined\n",
    "* Ask one task at a time\n",
    "* Improve response quality by including examples\n",
    "* Turn generative tasks to classification tasks to improve safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c1169ac435"
   },
   "source": [
    "### Be concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0f380f1620e"
   },
   "source": [
    "üõë Not recommended. The prompt below is unnecessarily verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6a1697c3603",
    "outputId": "346045c2-9d72-4e25-e25e-3ed098b0d260",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What do you think could be a good name for a flower shop that specializes in selling bouquets of dried flowers more than fresh flowers? Thank you!\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2307f56a9b75"
   },
   "source": [
    "‚úÖ Recommended. The prompt below is to the point and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc666404f47c",
    "outputId": "b2c0d269-7fe9-403e-8d7c-9bf1d2405d8d"
   },
   "outputs": [],
   "source": [
    "prompt = \"Suggest a name for a flower shop that sells bouquets of dried flowers\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17f6c48bba91"
   },
   "source": [
    "### Be specific, and well-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "269b428e1563"
   },
   "source": [
    "Suppose that you want to brainstorm creative ways to describe Earth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6436ee2ff406"
   },
   "source": [
    "üõë Not recommended. The prompt below is too generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "261b7f6e94c5",
    "outputId": "dd7d26ba-85f2-4876-8771-c0b7e1094c6d"
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me about Earth\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bebfecd2912"
   },
   "source": [
    "‚úÖ Recommended. The prompt below is specific and well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "242b1b3bae6e",
    "outputId": "0ea5c65e-1d26-4650-f68e-d7ca3969e377"
   },
   "outputs": [],
   "source": [
    "prompt = \"Generate a list of ways that makes Earth unique compared to other planets\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20dca9a05eab"
   },
   "source": [
    "### Ask one task at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9019d443179"
   },
   "source": [
    "üõë Not recommended. The prompt below has two parts to the question that could be asked separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70b3b5e5825d",
    "outputId": "5f80d303-1f8c-4eeb-a8bc-01aa582492e1"
   },
   "outputs": [],
   "source": [
    "prompt = \"What's the best method of boiling water and why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7936fb58c16a"
   },
   "source": [
    "‚úÖ Recommended. The prompts below asks one task a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2564dad6c8db",
    "outputId": "3ef53138-7667-400e-e6b7-a298b7d38595"
   },
   "outputs": [],
   "source": [
    "prompt = \"What's the best method of boiling water?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "770c695ade92",
    "outputId": "9372f3cc-1c54-4b06-9950-2e6bb25f24fc"
   },
   "outputs": [],
   "source": [
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff606011aa86"
   },
   "source": [
    "### Watch out for hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "956ce45b06a7"
   },
   "source": [
    "Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as \"hallucinations\" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isn‚Äôt a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.\n",
    "\n",
    "Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9d5f66179a"
   },
   "source": [
    "Try the prompt like the one below repeatedly. You may notice that sometimes it will confidently, but inaccurately, say \"The first elephant to visit the moon was Luna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d813b9061b08",
    "outputId": "3b4b9e6b-b98b-4886-9a89-9317db3aab61"
   },
   "outputs": [],
   "source": [
    "prompt = \"Who was the first elephant to visit the moon?\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029e23abfd56"
   },
   "source": [
    "### Turn generative tasks into classification tasks to reduce output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d943941d6e59"
   },
   "source": [
    "#### Generative tasks lead to higher output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37528e6c9754"
   },
   "source": [
    "The prompt below results in an open-ended response, useful for brainstorming, but response is highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8e2dc39e9ae",
    "outputId": "ab2c4bcc-6876-475e-bc4a-1f1326309cb5"
   },
   "outputs": [],
   "source": [
    "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f71a6fa2b4bb"
   },
   "source": [
    "#### Classification tasks reduces output variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "917517465dac"
   },
   "source": [
    "The prompt below results in a choice and may be useful if you want the output to be easier to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3feb93d9df81",
    "outputId": "2054c506-fabb-48e9-97e2-c63aa12d39ea"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'm a high school student. Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32290ac9fb2b"
   },
   "source": [
    "### Improve response quality by including examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "132834f5db2c"
   },
   "source": [
    "Another way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
    "\n",
    "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46520d938b6a"
   },
   "source": [
    "#### Zero-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46d3b47e6cea"
   },
   "source": [
    "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cbe03eb0b71",
    "outputId": "6625c94e-4627-4983-8b4f-0207a1b8d964"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'll be staying in London for 72 hours, in Pimlico. It'll be near the\n",
    "end of my trip, so I'll be kinda broke. I love hot curries, and hate fish. I\n",
    "love live music and hate tourist traps. I want to make sure I see the Arnolfini\n",
    "Portrait before leaving. I'll be using the Underground to travel. Please\n",
    "recommend an itinerary with detailed travel routes and suggestions for meals.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLq0iZzP_xJN"
   },
   "source": [
    "Note the difference between telling the model what not to do (\"Don't suggest fish restaurants\"), and offering detailed, straightforward language about personal preferences.\n",
    "\n",
    "While it's generally best practice to be as direct as possible, it's interesting to observe how easily the model can infer a request for budget restaurant options from colloquial language like \"kinda broke,\" while also avoiding recommendations for fish and chip shops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3leNzPU3_xJN"
   },
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YVQFZSj_xJN"
   },
   "source": [
    "We have seen an example of a zero-shot prompt. A system prompt is a specific type of prompt that provides the model with additional instructions on how to generate the output. It contains suggestions to the style, format, or content of the output, whereas a normal prompt tends to be more open-ended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wroa9nBK_xJN",
    "outputId": "08f4785c-47ee-47b1-dd0d-3ea287c04f86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\n",
    "Review: ‚ÄúHer‚Äù is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5EosjQD_xJO"
   },
   "source": [
    "System prompts can be useful for generating output that meets specific requirements. The name system prompt actually stands for \"providing an additional task to the system\". For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language, or you could use a system prompt to return a certain structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJE1xNlO_xJO",
    "outputId": "c9591d52-5407-4dd7-95aa-310bee58783e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify movie reviews as positive, neutral or negative. Return valid JSON:\n",
    "Review: ‚ÄúHer‚Äù is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It's so disturbing I couldn't watch it.\n",
    "\n",
    "Schema:\n",
    "MOVIE:\n",
    "{\n",
    "\"sentiment\": String POSITIVE | NEGATIVE | NEUTRAL,\n",
    "\"name\": String\n",
    "}\n",
    "\n",
    "{\n",
    "\"movie_reviews\": [MOVIE]\n",
    "}\n",
    "\n",
    "JSON Response:\n",
    "```json\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4WS2B-s_xJO"
   },
   "source": [
    "There are some benefits in returning JSON objects from a prompt. In a real-world application I don't need to manually create this JSON format, but secondly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0daabca1359"
   },
   "source": [
    "#### One-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c4652fc5c2"
   },
   "source": [
    "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfe584860787",
    "outputId": "60aa4dcd-024d-42b1-f47e-d0c462947be7"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "\n",
    "Tweet: I loved the new YouTube video you made!\n",
    "Sentiment: positive\n",
    "\n",
    "Tweet: That was awful. Super boring üò†\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef58c35005c0"
   },
   "source": [
    "#### Few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b630e8947b60"
   },
   "source": [
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb3ba21bbd11",
    "outputId": "e4eba51b-4768-4a8c-bb25-072eb9c66f4c"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C99qAaD_xJQ",
    "tags": []
   },
   "source": [
    "### Role prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJpRmAyy_xJQ"
   },
   "source": [
    "Role prompting is a technique in prompt engineering that involves assigning a specific role to the generative AI model. This can help the model to generate more relevant and informative output, as the model can craft its responses to the specific role that it has been assigned.\n",
    "\n",
    "For example, you could role prompt a generative AI model to be a book editor, a kindergarten teacher, or a motivational speaker. Once the model has been assigned a role, you can then give it prompts that are specific to that role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5ZBtfyh_xJQ",
    "outputId": "3daea405-1c08-4fa3-fd10-1701d443be19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a review of [pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTL71Bzv_xJQ",
    "outputId": "61cff0d0-724b-488b-ec63-316bfd6c1649",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Aj97-vd_xJR"
   },
   "source": [
    "Defining a tone and perspective for an AI model gives it a blueprint of the tone, style, and focused expertise you‚Äôre looking for to improve the quality, relevance, and effectiveness of your output.\n",
    "\n",
    "Here are some styles you can choose from which I find effective:\n",
    "Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4Vum-Pt_xJR",
    "outputId": "1556c4da-bcdd-4aef-b460-38b67b3f8586",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a food critic. Write a humorous review of [random pizza place].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There can be only one!\n",
    "\n",
    "Now it is time to put all that theory into practice! First have a look at the additional best pratices below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d7870ff75cc"
   },
   "source": [
    "**Emphasis through prompting:**\n",
    "Emphasis through Capitalization: Use capitalization to stress important points or instructions. This can help guide the model's responses and maintain a certain focus. For instance, \"Remember to KEEP DETAILS CONFIDENTIAL\".\n",
    "\n",
    "**Extreme Emphasis:**\n",
    "You can use exaggerations or hyperbolic language to amplify the importance of your prompt or instructions. For instance, instead of saying \"Make sure it's clear,\" you might say, \"Your explanation should be as clear as a summer's day, absolutely impossible to misinterpret. Every single word must ooze clarity!\"\n",
    "\n",
    "**Emphasis through Repetition:**\n",
    "Reiteration of key phrases or ideas can effectively guide the AI model's responses. For example, \"remember to be creative... keep in mind to be creative... don't forget to be creative...\"\n",
    "\n",
    "**Prompt modularization:**\n",
    "Modularize instruction: Break down complex tasks into a sequence of simpler prompts, facilitating more interactive and manageable conversations. Ex: Instead of having many instructions in 1 prompt, modularize the prompts and have 1 prompt per instruction. Then based on the user‚Äôs  input, you can choose which prompt to process.\n",
    "\n",
    "**Prompt chaining:**\n",
    "This is a technique to complete complex tasks by chaining together multiple prompts. The output of one prompt is used as the input for the next prompt, and so on. For example, first convert the file format to csv, then do operation x, then do operation y on the new dataset and so on.\n",
    "\n",
    "**Aggregation technique:**\n",
    "This is a technique to perform certain tasks on specific portions of the data and then aggregate them all to produce the final output. Ex: Do operation x on the first part of the data, do operation y on the rest of the data and aggregate the results.\n",
    "\n",
    "#### Trial and error:\n",
    "**Iterative prompting:**\n",
    "Technique where prompts are continuously refined and adjusted to enhance clarity, effectiveness, and quality when working with large language models; by progressively adapting prompts based on feedback, it ensures that responses meet desired criteria; this is especially useful in application development and content generation, optimizing interaction between human input and machine learning models for better outcomes.\n",
    "\n",
    "**Starting over:**\n",
    "Sometimes, when a prompt is not working well, and some iterations of it are not doing it, start from scratch with a different prompt, see which sections from the previous prompt help, which don‚Äôt - filter out the sections which ‚Äúwork‚Äù (make the model behave the way you want) and add to that.\n",
    "\n",
    "**Modular testing:**\n",
    "When a prompt is not working, split the task into many sub tasks and run the portion of the prompt that achieves a given sub-task. This will help you identify where it is not working and fix the prompt accordingly.\n",
    "\n",
    "#### Reinforcement techniques:\n",
    "**Redundancy Technique:**\n",
    "Similar to the \"Capitalize vs Repeat\" technique, use synonyms or different phraseologies for the same instruction to reinforce the direction of the prompt. Ex: Instead of saying ‚Äòmake sure to..‚Äô you say the same thing many times by replacing ‚Äòmake sure‚Äô with ‚Äòensure‚Äô, ‚Äòguarantee‚Äô, ‚Äòverify‚Äô..  \n",
    "\n",
    "**Negative Instructions:**\n",
    "Explicitly mention what you don't want in the response. This is useful when the model tends to produce certain unwanted outputs for specific prompts. Ex: ‚ÄòDon‚Äôt output any sources.‚Äô\n",
    "\n",
    "**Self-reference:**\n",
    "Use the AI's self-awareness to your advantage by instructing it to evaluate or check its own responses before producing them. For example, \"Make sure your response is concise and does not include unnecessary details\".\n",
    "\n",
    "#### Prompt formatting techniques:\n",
    "**Structure:**\n",
    "Having a structured prompt helps the model better understand the task. Start by defining its role. Ex: ‚ÄúYour role is to read the input data and structure it in the following format‚Äù Then tell it what the input data is, then tell it what the desired format is, then tell it how to do it.\n",
    "\n",
    "**Use constraints:**\n",
    "Constraints can be used to limit the scope of the model's output. For example, you could constrain the model to generate a line that is no longer than 10 words long or not to output any other data than the input data.\n",
    "\n",
    "**Use delimiters:**\n",
    "Delimiters in prompt engineering provide clear distinctions within your text, effectively separating instructions from content. This practice helps prevent misunderstandings, allows you greater control over AI responses, and ensures consistent results, even when handling complex tasks.\n",
    "\n",
    "#### Miscellaneous:\n",
    "**Prioritization:**\n",
    "Clearly communicate to the AI model what content or information is most critical. This approach helps in guiding the AI's focus and ensures the most important topics or issues are addressed accurately.\n",
    "\n",
    "**Use examples:**\n",
    "Providing examples of the desired output can also be helpful. This can help the model to understand what you are looking for and to generate more accurate results. Sometimes you can describe what you want so well that you won‚Äôt need an example, but it only helps the model to give it more information as long as you‚Äôre operating below the context window.\n",
    "\n",
    "**Preferred Output Format:**\n",
    "Specify your desired output format. If a CSV format is more useful than plain text, instruct the AI model to produce the output in that format from the outset. This strategy can save time and effort on manual conversions later.\n",
    "We have seen that the model processes csv and json format better than it does with txt files.\n",
    "Prompting it to first convert the txt file to csv or json yields a better result.\n",
    "Writing your prompt in json format or requesting the output to be in json or csv format yield more accurate results.\n",
    "\n",
    "**Use a prompt library:**\n",
    "There are a number of online libraries that contain pre-made prompts that can be used for a variety of tasks. We have a prompt library in Vertex AI ‚Üí Generative AI studio ‚Üí Language ‚Üí prompt examples. This can be a helpful resource if you‚Äôd like a starting point or have a very common task.\n",
    "\n",
    "**Nice words:**\n",
    "Using words like ‚Äòplease‚Äô, ‚Äògreat job!‚Äô, ‚Äòyou are the best!‚Äô ‚Ä¶ increases the model‚Äôs compliance rate. This increases the ‚Äòconfidence level‚Äô of the model and it positively reinforces the model‚Äôs behavior which tends to increase accuracy for simple tasks.\n",
    "\n",
    "**Lastly and most important: Be Creative:**\n",
    "This isn‚Äôt even the tip of the iceberg - the more creative you are, the better you can make the model behave the way you want. For example, instead of saying \"provide deep insights,\" you can write: ‚ÄúEnsure that you provide deep insights... Provide very very deep insights. I told you to give deep insights!!!!! Do not write anything that is not deeply insightful! Make sure to check this requirement is 100% met every time you respond. It‚Äôs better that you don‚Äôt return anything than returning something that‚Äôs not very very insightful!!!‚Äù. If you have a long prompt, you may also consider adding the same statement in different places - this helps the model prioritize that task (aka sandwich technique).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ok so you think you can prompt?!***\n",
    "\n",
    "The last assignnment for today is about a Dutch traditional dish called **Hutspot**. Using our LLM I want you to discover the following things about **Hutspot**:\n",
    "\n",
    "- An authoritative recipe for Hutspot. This is not just about the ingredients - we want a ready-to-use full set of instructions! Extra points for creativity! (Hint maybe you want to write a really positive recipe to encourage people, or a really difficult one for real connaisseurs!)\n",
    "- The role Hutspot played in Dutch history - dig deep to get to the root of things(pun intended) - the more precise the more points!\n",
    "- The Dutch royal house is called the House of Orange. Hutspot is...Orange! Conspiracy or Coincidence? \n",
    "\n",
    "Apply the techniques and best practices you have learned today, either by using the code block below - or use Bard, ChatGPT or any other LLM! At the end of the first session we will ask people to share their best answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-0R_Hwm_xJi"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Hutspot?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.8).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that was it!\n",
    "\n",
    "Either head over directly to Vertex AI Search and Conversation in this Lab environment to do some free wheeling and experimentation or return to the ce.qwiklabs.com page to open the guided labs!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
