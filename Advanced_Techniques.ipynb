{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mystic Art of Prompting - Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's install some packages, import stuff and define our generation model\n",
    "\n",
    "!pip install langchain\n",
    "!pip install google-search-results\n",
    "!pip install numexpr\n",
    "\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\") # if you are feeling experimental - try text-bison@002! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--RhKjxq_xJR"
   },
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvmEKAQH_xJS"
   },
   "source": [
    "Chain of Thought (CoT) [6] prompting is a technique which was first introduced in January 2022 for improving the reasoning capabilities of LLMs by letting them write intermediate reasoning steps. This helps the LLM understand the problem more deeply, and generate more accurate answers. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR9PAswd_xJS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets first look at the example without CoT\n",
    "\n",
    "prompt = \"\"\"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7atWE__D_xJS",
    "outputId": "ab1db245-dcbf-407d-b9af-063d694a625b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now, lets add CoT\n",
    "\n",
    "prompt = \"\"\" When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD8ADkOx_xJS"
   },
   "source": [
    "Chain of Thought prompting can be very powerful when combined with a single-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YV7XoOY_xJS",
    "outputId": "c70ef7ba-6385-4522-b788-e68eae5ac7f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now, lets add CoT\n",
    "\n",
    "prompt = \"\"\" Q: When my brother was 4 years old, I was double his age. Now I am 40 years old. How old is my brother? Let's think step by step.\n",
    "A: When my brother was 4 years, I was 2 * 2 = 8 years old. That's an age difference of 4 years and I am older. Now I am 40 years old, so my brother is 40 - 4  = 36 years old. The answer is 36.\n",
    "Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let's think step by step.\n",
    "A:\n",
    "\"\"\"\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSo1LyG4_xJT"
   },
   "source": [
    "### Tree of Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxMRDQCE_xJT"
   },
   "source": [
    "Now that we are familiar with Chain of Thoughts, let's review Tree of Thoughts (ToT) , which was first introduced in May 2023. It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought.\n",
    "\n",
    "![Tree of Thoughts](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&amp;w=3840&amp;q=75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c3cI279_xJT",
    "outputId": "08918b0f-5e90-40c2-95cc-c1488290cf47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"The question is: \"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\"\n",
    "\n",
    "Now imagine 3 different experts are answering this question.\n",
    "All experts will write down the steps of their thinking, by calculating the age difference back then to calculate the current age of the partner. If any expert realizes they're wrong at any point then they leave.\n",
    "The correct answer is the answer that has been answered the most.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVn5LKlw_xJU"
   },
   "source": [
    "### Reason and Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7s0YmDV_xJU"
   },
   "source": [
    "Reason and Act (ReAct) [8] prompting was first introduced in October 2022 and is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning. It is designed for tasks where the LLM is allowed to perform certain actions, such as interacting with external APIs to retrieve information.\n",
    "ReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct performs well against other prompt engineering approaches in a variety of domains.\n",
    "\n",
    "ReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEMscnuU_xJU",
    "outputId": "da7755f8-2f74-46a0-b039-3e0f68659c52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "#Create a SERPAPI_API_KEY on https://serpapi.com/ for free to use (amongst others) the Google Search API\n",
    "SER_API_KEY = \"[YOUR_SERPAPI_API_KEY]\"\n",
    "import os\n",
    "os.environ['SERPAPI_API_KEY']=str(SER_API_KEY)\n",
    "\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "\n",
    "llm = VertexAI(temperature=0.1)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm,  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2QxaJ1E_xJU",
    "tags": []
   },
   "source": [
    "### Automatic Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SINmq49w_xJV"
   },
   "source": [
    "At this point you might realize that writing a prompt can be complex and. Wouldn't it be possible to automate this (write a prompt to write prompts) and take out natural language ambiguity? Well, there's a method: Automatic Prompt Engineering (APE). . This method - described in a 2023 research paper  - not only alleviates the need for human input but also enhances the model’s performance in various tasks.\n",
    "\n",
    "The idea is at follows:\n",
    "\n",
    "You pick a task that needs to automate the prompt for crafting and optimizing the best prompt for you.\n",
    "\n",
    "In our example, we are building a shopping assistant for a retail company. We want to figure out all the various ways customers could phrase their order for buying a new coat.\n",
    "\n",
    "Write the prompt which will generate the instruction candidates. You can do this through an LLM, but there are also various hubs and collections of prompts that you can find online. In this example, I am using Text-Bison-32k to generate 10 instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddIjsstT_xJV",
    "outputId": "f2d52f91-2961-4196-fb98-a5d6e731b2ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"We have a retail webshop, and to train a chatbot we need various ways to order: \"Coat, black, puffer jacket size M\". Generate 10 variants, with the same semantics but keep the same meaning.\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx2p1k-N_xJV"
   },
   "source": [
    "Now it's your turn! Create a prompt to create a instruction candidate for the perfect prompt for your specific problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"YOUR PROMPT GOES HERE!\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024).text)\n",
    "\n",
    "better_prompt=(generation_model.predict(prompt=prompt, max_output_tokens=1024).text)\n",
    "\n",
    "print(generation_model.predict(prompt=better_prompt, max_output_tokens=1024).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIk7elmT_xJV"
   },
   "source": [
    "### Take a step-back prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnwBnL1O_xJV"
   },
   "source": [
    "As we have seen with most prompting techniques published, Large Language Models (LLMs) need guidance when intricate, multi-step reasoning is demanded from a query, and decomposition is a key component when solving complex request.\n",
    "\n",
    "A process of supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning step\n",
    "\n",
    "The most well known prompting technique when it comes to decomposition is chain-of-thought reasoning. In this study Step-Back Prompting is compared to COT prompting.\n",
    "\n",
    "The text below shows a complete example of STP with the original question, the stepback question, principles, and the prompt for the final answer to be generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5DedDkn_xJW",
    "outputId": "c649675f-f832-459e-9636-6f7eee11250c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This shows example of the stepback question that is generated\n",
    "\n",
    "prompt = \"\"\" You are an expert at world knowledge.\n",
    "Your task is to step back and paraphrase a question to a more generic\n",
    "step-back question, which is easier to answer.\n",
    "\n",
    "Here are a few examples:\n",
    "Original Question: Which position did Knox Cunningham hold from May 1955 to Apr 1956?\n",
    "Stepback Question: Which positions have Knox Cunning- ham held in his career?\n",
    "\n",
    "Original Question: Who was the spouse of Anna Karina from 1968 to 1974?\n",
    "Stepback Question: Who were the spouses of Anna Karina?\n",
    "\n",
    "Original Question: Which team did Thierry Audel play for from 2007 to 2008?\n",
    "Stepback Question: Which teams did Thierry Audel play for in his career?\n",
    "\n",
    "Question: Was donald trump president when Palm was announced?\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prompt=prompt, max_output_tokens=1024, temperature=0.1).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu47Dtdh_xJW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXNwsS92_xJW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"did Google Bard exist while Trump was president? Answer with yes or no and provide your explanation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmqykbWA_xJg",
    "outputId": "8d1e1561-ce76-4684-bf99-196a0e4ddb5f"
   },
   "outputs": [],
   "source": [
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "{normal_context}\n",
    "\n",
    "Original Question: {question}\n",
    "Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "######################################\n",
    "\n",
    "chain = {\n",
    "    # Retrieve context using the normal question\n",
    "    \"normal_context\": RunnableLambda(lambda x: x['question']) | retriever,\n",
    "    # Pass on the question\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | response_prompt | ChatVertexAI(temperature=0) | StrOutputParser()\n",
    "\n",
    "######################################\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There can be only one!\n",
    "\n",
    "Now it is time to put all that theory into practice! First have a look at the additional best pratices below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emphasis through prompting:**\n",
    "Emphasis through Capitalization: Use capitalization to stress important points or instructions. This can help guide the model's responses and maintain a certain focus. For instance, \"Remember to KEEP DETAILS CONFIDENTIAL\".\n",
    "\n",
    "**Extreme Emphasis:**\n",
    "You can use exaggerations or hyperbolic language to amplify the importance of your prompt or instructions. For instance, instead of saying \"Make sure it's clear,\" you might say, \"Your explanation should be as clear as a summer's day, absolutely impossible to misinterpret. Every single word must ooze clarity!\"\n",
    "\n",
    "**Emphasis through Repetition:**\n",
    "Reiteration of key phrases or ideas can effectively guide the AI model's responses. For example, \"remember to be creative... keep in mind to be creative... don't forget to be creative...\"\n",
    "\n",
    "**Prompt modularization:**\n",
    "Modularize instruction: Break down complex tasks into a sequence of simpler prompts, facilitating more interactive and manageable conversations. Ex: Instead of having many instructions in 1 prompt, modularize the prompts and have 1 prompt per instruction. Then based on the user’s  input, you can choose which prompt to process.\n",
    "\n",
    "**Prompt chaining:**\n",
    "This is a technique to complete complex tasks by chaining together multiple prompts. The output of one prompt is used as the input for the next prompt, and so on. For example, first convert the file format to csv, then do operation x, then do operation y on the new dataset and so on.\n",
    "\n",
    "**Aggregation technique:**\n",
    "This is a technique to perform certain tasks on specific portions of the data and then aggregate them all to produce the final output. Ex: Do operation x on the first part of the data, do operation y on the rest of the data and aggregate the results.\n",
    "\n",
    "#### Trial and error:\n",
    "**Iterative prompting:**\n",
    "Technique where prompts are continuously refined and adjusted to enhance clarity, effectiveness, and quality when working with large language models; by progressively adapting prompts based on feedback, it ensures that responses meet desired criteria; this is especially useful in application development and content generation, optimizing interaction between human input and machine learning models for better outcomes.\n",
    "\n",
    "**Starting over:**\n",
    "Sometimes, when a prompt is not working well, and some iterations of it are not doing it, start from scratch with a different prompt, see which sections from the previous prompt help, which don’t - filter out the sections which “work” (make the model behave the way you want) and add to that.\n",
    "\n",
    "**Modular testing:**\n",
    "When a prompt is not working, split the task into many sub tasks and run the portion of the prompt that achieves a given sub-task. This will help you identify where it is not working and fix the prompt accordingly.\n",
    "\n",
    "#### Reinforcement techniques:\n",
    "**Redundancy Technique:**\n",
    "Similar to the \"Capitalize vs Repeat\" technique, use synonyms or different phraseologies for the same instruction to reinforce the direction of the prompt. Ex: Instead of saying ‘make sure to..’ you say the same thing many times by replacing ‘make sure’ with ‘ensure’, ‘guarantee’, ‘verify’..  \n",
    "\n",
    "**Negative Instructions:**\n",
    "Explicitly mention what you don't want in the response. This is useful when the model tends to produce certain unwanted outputs for specific prompts. Ex: ‘Don’t output any sources.’\n",
    "\n",
    "**Self-reference:**\n",
    "Use the AI's self-awareness to your advantage by instructing it to evaluate or check its own responses before producing them. For example, \"Make sure your response is concise and does not include unnecessary details\".\n",
    "\n",
    "#### Prompt formatting techniques:\n",
    "**Structure:**\n",
    "Having a structured prompt helps the model better understand the task. Start by defining its role. Ex: “Your role is to read the input data and structure it in the following format” Then tell it what the input data is, then tell it what the desired format is, then tell it how to do it.\n",
    "\n",
    "**Use constraints:**\n",
    "Constraints can be used to limit the scope of the model's output. For example, you could constrain the model to generate a line that is no longer than 10 words long or not to output any other data than the input data.\n",
    "\n",
    "**Use delimiters:**\n",
    "Delimiters in prompt engineering provide clear distinctions within your text, effectively separating instructions from content. This practice helps prevent misunderstandings, allows you greater control over AI responses, and ensures consistent results, even when handling complex tasks.\n",
    "\n",
    "#### Use LLMs (ex: Bard) to build prompts:\n",
    "**Refining Prompts:**\n",
    "By providing an initial prompt and the output received, you can ask the LLM to refine the prompt to achieve a more desired output. The LLM can provide insight into what might be improved, suggest changes\n",
    "\n",
    "**Generating New Prompts:**\n",
    "If you provide the LLM with a set of requirements, desired output format, and examples of \"requirement, prompt\" tuples, it can generate a new prompt based on the given requirements. This could be useful when creating prompts for new scenarios or tasks.\n",
    "\n",
    "**Feedback:**\n",
    "Once you have a prompt, you can ask the LLM for feedback. This can help you to identify areas where the prompt could be improved. The LLm can help you point out some gaps in your prompt engineering skills based on your prompt and what you want to achieve.\n",
    "\n",
    "#### Miscellaneous:\n",
    "**Prioritization:**\n",
    "Clearly communicate to the AI model what content or information is most critical. This approach helps in guiding the AI's focus and ensures the most important topics or issues are addressed accurately.\n",
    "\n",
    "**Use examples:**\n",
    "Providing examples of the desired output can also be helpful. This can help the model to understand what you are looking for and to generate more accurate results. Sometimes you can describe what you want so well that you won’t need an example, but it only helps the model to give it more information as long as you’re operating below the context window.\n",
    "\n",
    "**Preferred Output Format:**\n",
    "Specify your desired output format. If a CSV format is more useful than plain text, instruct the AI model to produce the output in that format from the outset. This strategy can save time and effort on manual conversions later.\n",
    "We have seen that the model processes csv and json format better than it does with txt files.\n",
    "Prompting it to first convert the txt file to csv or json yields a better result.\n",
    "Writing your prompt in json format or requesting the output to be in json or csv format yield more accurate results.\n",
    "\n",
    "**Use a prompt library:**\n",
    "There are a number of online libraries that contain pre-made prompts that can be used for a variety of tasks. We have a prompt library in Vertex AI → Generative AI studio → Language → prompt examples. This can be a helpful resource if you’d like a starting point or have a very common task.\n",
    "\n",
    "**Nice words:**\n",
    "Using words like ‘please’, ‘great job!’, ‘you are the best!’ … increases the model’s compliance rate. This increases the ‘confidence level’ of the model and it positively reinforces the model’s behavior which tends to increase accuracy for simple tasks.\n",
    "\n",
    "**Lastly and most important: Be Creative:**\n",
    "This isn’t even the tip of the iceberg - the more creative you are, the better you can make the model behave the way you want. For example, instead of saying \"provide deep insights,\" you can write: “Ensure that you provide deep insights... Provide very very deep insights. I told you to give deep insights!!!!! Do not write anything that is not deeply insightful! Make sure to check this requirement is 100% met every time you respond. It’s better that you don’t return anything than returning something that’s not very very insightful!!!”. If you have a long prompt, you may also consider adding the same statement in different places - this helps the model prioritize that task (aka sandwich technique).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erswna2F_xJi"
   },
   "source": [
    "## Additional Exercises\n",
    "\n",
    "\n",
    "- Try out https://gandalf.lakera.ai/  Cannot find the password? Tips: https://medium.com/the-abcs-of-ai/gandalfs-challenge-mastering-prompt-engineering-for-ai-success-fd777be2aa0b\n",
    "- Try out the examples"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
